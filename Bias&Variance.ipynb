{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias & Variance\n",
    "\n",
    "Let's talk about bias and variance. These are main factors that we have to consider when we evaluate model.\n",
    "\n",
    "It can be explained with overfitting and underfitting. So, I am going to cover bias and variance with overfitting and underfitting,\n",
    "\n",
    "and how to deal with this problem.\n",
    "\n",
    "## 1. Error\n",
    "\n",
    "In machine learning model, error consists of 'noise', 'bias' and 'variance'. Noise is its own intrinsic error that we can't handle. \n",
    "Bias is tendency that learning mistakenly by not considering all information from data, called underfitting.\n",
    "On the other hand, variace is tendency that learning everything including random value or noise which is not general in practical problem, which is called overfitting.\n",
    "We can say that linear model is under-fit and multinomial that has high degree is over-fit.\n",
    "There is trade off between bias and variance. So, we need to find optimum model to generate minimum 'total error'.\n",
    "\n",
    "## 2. How to handle \n",
    "\n",
    "First, we need to set three things, human level, train error and dev error. Gap between human level and trade error is bias, and train error and dev error is variance. \n",
    "\n",
    "Usually, to fix high bias problem, we make bigger networks or iterate more number or tune other hyperparameters of model. But at some points, we couldn't get lower bias and variance would be getting higher. Hence, we need to 'regulate' model. Though, feeding more data for model is best way to lower variance but it is quite expensive and sometimes, it is not possible. To figure out this, there are some methods to handle this.\n",
    "\n",
    "### 2.1 Reduction of networks\n",
    "\n",
    "The bigger the networks, the more chance to find more complex pattern, but prone to get high variance, overfitting problem. We have to find proper number of hidden layers or nodes. Unfortunately, there are not equation to get proper number of those. \n",
    "\n",
    "So, common flow of finding proper size of networks is starting from smaller hyperparameters, evaluate and increase number of hyperparameters till find increase of dev set error, gap between train and dev error.\n",
    "\n",
    "### 2.2 Weight regularization\n",
    "\n",
    "Simpler model is more likely not to be overfitting. Model consists of lots of values of weights. What if we regularize weights of model? It means suppress the degree of model and finally, we can get simpler model. \n",
    "\n",
    "There are two methods to regularize weights, L1, L2 and elastic which is mix of them. Hyperparameter of weight regularization is 'alpha'. (Bigger alpha means stronger regularization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "regularizers.l1(0.001)\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dropout\n",
    "\n",
    "Dropout method is invented by Hinton at Toronto University. He invented it based on idea of fraud prevention of banks. I think more than its idea, that it works well for regularization anyway, is more important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
