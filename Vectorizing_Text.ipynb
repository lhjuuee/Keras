{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data\n",
    "## 1. Vectorizing text\n",
    "\n",
    "There are three ways to represent text as number.\n",
    "\n",
    "- Text -> Words -> Vector\n",
    "- Text -> Character -> Vector\n",
    "- Text -> n-gram -> Vector\n",
    "\n",
    "**n-gram means subset of n words.** used for shallow algorithms.\n",
    "\n",
    "And these units, like words, character and n-gram, are **token**. So, we can say it is done by text -> token(ize) -> vector.\n",
    "\n",
    "To transform token to vector, there are two ways, one-hot or word embedding.\n",
    "\n",
    "### 1.1 One-hot encoding\n",
    "\n",
    "There is keras utility to encoding. It is much more comfortable and It has lots of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000) # Number of workds in dictionary (most used 1000)\n",
    "tokenizer.fit_on_texts(samples) # Create index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples) \n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'mat': 5,\n",
       " 'dog': 6,\n",
       " 'ate': 7,\n",
       " 'my': 8,\n",
       " 'homework': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one-hot hashing, Variation of one-hot encoding\n",
    "\n",
    "It is used when we have too large dictionary to use.\n",
    "\n",
    "### 1.2 Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is most powerful and popular method to vectorize text. In one-hot encoding, we have lots of 0 elements in matrix which\n",
    "\n",
    "means sparse and high dimension. We can handle data more efficiently. (Dense <-> Sparse)\n",
    "\n",
    "\n",
    "- Usually, we have 256, 512 or 1024 dimension of word embedding in project. If we try to use one-hot encoding, dimension of dictionary would be more than 20,000.\n",
    "\n",
    "- And it feeds data directly, not like one-hot vector.\n",
    "\n",
    "#### How to use word embedding \n",
    "\n",
    "##### Build and train own Word embedding & Pre-trained word embedding.\n",
    "\n",
    "There are two ways to build word embedding.\n",
    "\n",
    "- In project we try to solve, train word embedding like neural network\n",
    "\n",
    "- Use pretrained word embedding used for other projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Train word embedding \n",
    "\n",
    "Embedding features its own way to mapping words with relationship. Using this relationship, like King + Female = Queen, King + Plural = Kings, \n",
    "\n",
    "word embedding represent lots of words.\n",
    "\n",
    "In technology so far, even in real human, can't accurately map all the words due to its differnce for each country or culture.\n",
    "\n",
    "Hence, it is quite reasonable to train new word embedding in new projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64) # Word index 1~999 + 1 and embedding dimension 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer gets input as (samples, sequence_length) 2D tensor.(Integer)\n",
    "sample, shorter than sequence_length, would be filled with 0 and longer than sequence_length, would be cut.\n",
    "\n",
    "Embedding layer returns output as (samples, sequence_length, embedding_dimensionalyity) 3D tensor.(Real number)\n",
    "\n",
    "Embedding layer is initialized randomly and adjusted by back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000 # Number of words for feature\n",
    "maxlen = 20 # Length of text to use (most used)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "\n",
    "# Transform list(Text) into 2D tensor (samples, maxlen)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  23,    4, 1690,   15,   16,    4, 1355,    5,   28,    6,   52,\n",
       "        154,  462,   33,   89,   78,  285,   16,  145,   95])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen)) # Output shape is (samples, maxlen, 8)\n",
    "\n",
    "model.add(Flatten()) # Unroll 3D embedding tensor to 2D tensor (samples, maxlen * 8)\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 160us/step - loss: 0.6759 - acc: 0.6042 - val_loss: 0.6398 - val_acc: 0.6808\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.5657 - acc: 0.7428 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 148us/step - loss: 0.4263 - acc: 0.8079 - val_loss: 0.5008 - val_acc: 0.7454\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3930 - acc: 0.8257 - val_loss: 0.4981 - val_acc: 0.7540\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5013 - val_acc: 0.7534\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 149us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5051 - val_acc: 0.7516\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 146us/step - loss: 0.3023 - acc: 0.8765 - val_loss: 0.5213 - val_acc: 0.7494\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5302 - val_acc: 0.7468\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns 75% val accuracy. Given that I used only 20 words in samples, It is quite good result.\n",
    "\n",
    "But it can't consider relationship between words or structure of sentence. To figure out this, we need to add 1D or RNN layer\n",
    "\n",
    "on the embedding layer. (Talk about later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Pretrained word embedding\n",
    "\n",
    "Like pretrained CNN, we can use pretrained word embedding when we have little datasets for problem.\n",
    "\n",
    "Word2Vec or GloVe are famous embedding. Let's see how it works in next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
